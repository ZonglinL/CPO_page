<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="NeurIPS2025">
  <meta property="og:title" content="Diffusoin"/>
  <meta property="og:description" content="Diffusion"/>
  <!--<meta property="og:url" content="https://zonglinl.github.io/CPO_page/"/>-->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <style>
      .item img {
          display: block;
          margin-left: auto;
          margin-right: auto;
          margin-up: auto;
          margin-down: auto;
      }
  </style>
  <title>CPO: Condition Preference Optimization for Controllable Image Generation</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <!--<link rel="stylesheet" href="static/css/fontawesome.all.min.css">-->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <!--<script defer src="static/js/fontawesome.all.min.js"></script>-->
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">CPO: Condition Preference Optimization forControllable Image Generation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://zonglinl.github.io/" target="_blank">Zonglin Lyu</a>,</span>
                <span class="author-block">
                <a href="https://liming-ai.github.io/" target="_blank">Ming Li</a>,</span>
                <span class="author-block">
                <a href="https://openreview.net/profile?id=~Xinxin_Liu4" target="_blank">Xnxin Liu</a>,</span>
                  <span class="author-block">
                    <a href="https://www.crcv.ucf.edu/chenchen/" target="_blank">Chen Chen</a>
                  </span>
          
                <span class="author-block">
                  </div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institute of Artificial Intelligence, University of Central Florida<br> <b>NeurIPS 2025</b></span>
                  </div>
                  
                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="TODO" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                      </>
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ZonglinL/CPO" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>


                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="TODO" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <iframe height="480" width="960" 
            src="TODO"> 
      </iframe> 
    </div>
  </div>
</section>
<!-- End teaser video -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>To enhance controllability in text-to-image generation, ControlNet introduces image-based control signals, while ControlNet++ improves pixel-level cycle consistency between generated images and the input control signal. To avoid the prohibitive cost of back-propagating through the sampling process, ControlNet++ optimizes only low-noise timesteps (e.g., t < 200) using a single-step approximation, which not only ignores the contribution of high-noise timesteps but also introduces additional approximation errors. A straightforward alternative for optimizing controllability across all timesteps is Direct Preference Optimization (DPO), a fine-tuning method that increases model preference for more controllable images (I<sup>w</sup>}) over less controllable ones (I^<sup>l</sup>). However, due to uncertainty in generative models, it is difficult to ensure that win--lose image pairs differ only in controllability while keeping other factors, such as image quality, fixed. To address this, we propose performing preference learning over control conditions rather than generated images. Specifically, we construct winning and losing control signals, <b>c<sup>w</sup></b> and <b>c<sup>l</sup></b> , and train the model to prefer  <b>c<sup>w</sup></b>. This method, which we term \textit{Condition Preference Optimization} (CPO), eliminates confounding factors and yields a low-variance training objective. Our approach theoretically exhibits lower contrastive loss variance than DPO and empirically achieves superior results. Moreover, CPO requires less computation and storage for dataset curation. Extensive experiments show that CPO significantly improves controllability over the state-of-the-art ControlNet++ across multiple control types: over 10% error rate reduction in segmentation, 70 -- 80% in human pose, and consistent 2%--5% reductions in edge and depth maps. Here, the error rate is defined as the difference between the evaluated controllability and the oracle results.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Overview image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
              Instead of Contrasting images, our proposed method contrast conditions directly. As a result, the model sees a clear contrasting signal from the conditions and learns to improve the controllabiliy.
          </p>
        </div>
      </div>
    </div>
    <div class="hero-body">
    <img src="static/images/overview.png" alt="overview"/>
      </img>
      <h2 class="subtitle has-text-centered">
        Overview of our method. (a) Training autoencoder. The autoencoder is trained with video clip V = [I<sub>0</sub>, I<sub>n</sub>, I<sub>1</sub>] and aims to reconstruct I<sub>n</sub>. It contains an image encoder (shared for all frames) and an image decoder, where multi-level encoder features from I<sub>0</sub>, I<sub>1</sub> are passed to the decoder. Temporal blocks extract temporal information in the latent space and aggregate video features into a single image feature for the image decoder, and 3D Wavelet extracts temporal information in the pixel space. (b) Training Denoising UNet. The video clip V is encoded to x<sub>0</sub> by Encoder E (spatial + temporal). Since I<sub>n</sub> is unknown, we replace it by 0 and obtain another video clip V&#x0303 = [I<sub>0</sub>, 0, I<sub>1</sub>], which is encoded to x<sub>T</sub>. With the Brownian Bridge Diffusion Process, x<sub>t</sub> is computed and sent to denoising UNet to predict x<sub>t</sub> âˆ’ x<sub>0</sub>. (c) Inference. During inference, we encode V&#x0303 to x<sub>T</sub> and sample with the Brownian Bridge Sampling Process to get xÌ‚<sub>0</sub>, which is decoded to the output frame IÌ‚<sub>n</sub>.
      </h2>
    </div>
  </div>
</section>
<!-- End overview image -->



<!-- Architecture image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Architecture of The Autoencoder</h2>
        <div class="content has-text-justified">
          <p>
            Our autoencoder takes advantage of temporal information and optical flow to guide reconstruction.
          </p>
        </div>
      </div>
    </div>
    <div class="hero-body">
    <img src="static/images/Arch.jpg" alt="arch"/>
      </img>
      <h2 class="subtitle has-text-centered">
      (a) Model Pipeline. The Image Encoder is shared across all frames, and temporal blocks extract temporal information in the latent space. (b) Multi-level Feature Sharing. The Image Encoder and Decoder consist of several levels of resolution due to downsampling/upsampling latent features. At the ith level of the encoder and the decoder, features from I0 and I1 in the encoder are warped and concatenated with the original copy (when the downsampling rate is larger than 8, warped features are excluded). The concatenated features are used as keys and values in cross attention where the decoder feature at the same level is the query. (c) Encoder/Decoder Temporal Block. Each temporal block consists of two sets of 3D convolution + attention. In the decoder, the second attention is cross-attention between the intermediate frame (query) and all frames (key and value) to aggregate the video feature into one feature map. (d) 3D-wavelet Feature Gating. Wavelet information is extracted from the input video clip and encoded by CNNs. A sigmoid activation is applied, and the result is element-wise multiplied by the output of the Image Encoder with a skip connection.
      </h2>
    </div>
  </div>
</section>
<!-- End Architecture image -->
  

<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
          </p>
        </div>
      </div>
    </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/quant.png" alt="quantitative"/>
        <h2 class="subtitle has-text-centered">
          Quantitative Comparison between our method and recent SOTAs in LPIPS/FloLPIPS/FID (the lower the better). The best performance are <b>boldfaced</b>, and the second best performance are <u>underlined</u>.  Click the "next" icon to see the qualitative comparison between our method and recent SOTAs.
        </h2>
        </div>

       <div class="item">
        <!-- Your image here -->
        <img src="static/images/visual1.png" alt="sota"/>
        <h2 class="subtitle has-text-centered">
          Qualitative comparison between our method and recent SOTAs. Images cropped in <font color="blue"> blue boxes </font> are displayed for comparison, and <font color = "red">red circles</font> highlight our stronger performance. Click the "next" icon to see additional qualitative results.
        </h2>
      </div>

        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/visual2.png" alt="sota"/>
        <h2 class="subtitle has-text-centered">
          Additional Qualitative comparison between our method and recent SOTAs. Images cropped in <font color="blue"> blue boxes </font> are displayed for comparison, and <font color = "red">red circles</font> highlight our stronger performance. Click the "next" icon to see additional qualitative results.
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/visual3.png" width="90%" alt="ldm"/>
        <h2 class="subtitle has-text-centered">
         Additional Qualitative comparison between our method and recent SOTAs. Images cropped in <font color="blue"> blue boxes </font> are displayed for comparison, and <font color = "red">red circles</font> highlight our stronger performance.
        </h2>
  </div>
</div>
</div>
</section>
<!-- End Results -->
  
<!-- Multiframe -->

<!-- Image carousel -->
<section class="hero is-small">
    <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-frame Interpolation</h2>
        <div class="content has-text-justified">
          <p>
          We interpolate 7 frames between two frames and visualize the generated video with <b>our method</b> and <b>PerVFI</b>. Click "next" icon to view more images.
          </p>
        </div>
      </div>
    </div>
    </div>
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_0_im_0.png" alt="vid0im1"/>
        <h2 class="subtitle has-text-centered">
          The starting frame of multiframe interpolation. Click the "next" icon to see the ending frame.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_0_im_1.png" alt="vid0im1"/>
        <h2 class="subtitle has-text-centered">
          The ending frame of multiframe interpolation. Click the "next" icon to see our interpolation result.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/our0.gif" alt="our0"/>
        <h2 class="subtitle has-text-centered">
          Multiframe interpolation result of <b>our method</b>. The first and the last frame are given, the middle 7 frames are interpolated. Click the "next" icon to see the result of PerVFI
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/per0.gif" alt="per0"/>
        <h2 class="subtitle has-text-centered">
          Multi-frame interpolation result of <b>PerVFI</b>. The first and the last frame are given, the middle 7 frames are interpolated. The tires are distorted.
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->
  

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_1_im_0.png" alt="vid2im1"/>
        <h2 class="subtitle has-text-centered">
          The starting frame of multiframe interpolation. Click the "next" icon to see the ending frame.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_1_im_1.png" alt="vid2im1"/>
        <h2 class="subtitle has-text-centered">
          The ending frame of multiframe interpolation. Click the "next" icon to see our interpolation result.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/our1.gif" alt="our2"/>
        <h2 class="subtitle has-text-centered">
          Multiframe interpolation result of <b>our method</b>. The first and the last frame are given, the middle 7 frames are interpolated. Click the "next" icon to see the result of PerVFI
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/per1.gif" alt="ldm2"/>
        <h2 class="subtitle has-text-centered">
          Multi-frame interpolation result of <b>PerVFI</b>. The first and the last frame are given, the middle 7 frames are interpolated. The wings are distorted.
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_2_im_0.png" alt="vid3im0"/>
        <h2 class="subtitle has-text-centered">
          The starting frame of multiframe interpolation. Click the "next" icon to see the ending frame.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_2_im_1.png" alt="vid2im1"/>
        <h2 class="subtitle has-text-centered">
          The ending frame of multiframe interpolation. Click the "next" icon to see our interpolation result.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/our2.gif" alt="our3"/>
        <h2 class="subtitle has-text-centered">
          Multiframe interpolation result of <b>our method</b>. The first and the last frame are given, the middle 7 frames are interpolated. Click the "next" icon to see the result of PerVFI
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/per2.gif" alt="ldm3"/>
        <h2 class="subtitle has-text-centered">
          Multi-frame interpolation result of <b>PerVFI</b>. The first and the last frame are given, the middle 7 frames are interpolated. The hand is distorted.
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

  
<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_3_im_0.png" alt="vid1im1"/>
        <h2 class="subtitle has-text-centered">
          The starting frame of multiframe interpolation. Click the "next" icon to see the ending frame.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/vid_3_im_1.png" alt="vid0im1"/>
        <h2 class="subtitle has-text-centered">
          The ending frame of multiframe interpolation. Click the "next" icon to see our interpolation result.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/our3.gif" alt="our1"/>
        <h2 class="subtitle has-text-centered">
          Multiframe interpolation result of <b>our method</b>. The first and the last frame are given, the middle 7 frames are interpolated. Click the "next" icon to see the result of PerVFI
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/per3.gif" alt="ldm1"/>
        <h2 class="subtitle has-text-centered">
          Multi-frame interpolation result of <b>PerVFI</b>. The first and the last frame are given, the middle 7 frames are interpolated. The eye is distorted.
        </h2>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<h1 class="subtitle has-text-centered">
<b>More Multiframe Interpolation Visualization of our method.</b> We interpolate 7 frames between two frames.
</h1>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" >
        <!-- Your image here -->
        <img src="static/images/our4.gif" alt="our4" />
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/our5.gif" alt="our5" class="center"/>
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/our6.gif" alt="our6" class="center"/>
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
      </div>
          
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/our7.gif" alt="our7" class="center"/>
        <h2 class="subtitle has-text-centered">
         Click the "next" icon to see more.
        </h2>
      </div>


        <div class="item" >
        <!-- Your image here -->
        <img src="static/images/new0.gif" alt="new0" />
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
        </div>

        <div class="item">
        <!-- Your image here -->
        <img src="static/images/new1.gif" alt="new1" class="center"/>
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
        </div>
        
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/new2.gif" alt="new2" class="center"/>
        <h2 class="subtitle has-text-centered">
          Click the "next" icon to see more.
        </h2>
      </div>
          
</div>
</div>
</section>



  
<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
